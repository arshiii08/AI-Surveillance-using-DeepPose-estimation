{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arshiii08/AI-Surveillance-using-DeepPose-estimation/blob/main/Minor_Project_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOVZXbT5H3Dv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import clip  # OpenAI CLIP package\n",
        "from tqdm import tqdm  # For progress bar with ETA\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Setup: Paths and Device\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "image\\_dir = 'C:/Users/devel/Desktop/DensePose/train2014/train2014'                # Folder containing your images (update as needed)\n",
        "output\\_file = 'train2014\\_activity\\_annotated.json'  # Output JSON file for pseudo-labels\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is\\_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Load the CLIP Model\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# We use the \"ViT-B/32\" CLIP model along with its preprocessing function.\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Define Candidate Activity Labels\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Update this list with your desired activity categories.\n",
        "\n",
        "activity\\_labels = \\[\n",
        "\"standing\",\n",
        "\"sitting\",\n",
        "\"walking\",\n",
        "\"running\",\n",
        "\"jumping\",\n",
        "\"lying down\",\n",
        "\"bending\"\n",
        "]\n",
        "\n",
        "# Create text prompts for zero-shot classification\n",
        "\n",
        "text\\_prompts = \\[f\"a photo of a person {label}\" for label in activity\\_labels]\n",
        "text\\_tokens = clip.tokenize(text\\_prompts).to(device)\n",
        "\n",
        "# Create person count detection prompts\n",
        "\n",
        "person\\_count\\_prompts = \\[\n",
        "\"a photo of a single person\",\n",
        "\"a photo of multiple people\",\n",
        "\"a photo with no people\"\n",
        "]\n",
        "person\\_count\\_tokens = clip.tokenize(person\\_count\\_prompts).to(device)\n",
        "\n",
        "# Create activity clarity prompts\n",
        "\n",
        "activity\\_clarity\\_prompts = \\[\n",
        "\"a photo with a clearly identifiable human activity\",\n",
        "\"a photo with an ambiguous or unclear human activity\"\n",
        "]\n",
        "activity\\_clarity\\_tokens = clip.tokenize(activity\\_clarity\\_prompts).to(device)\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Initialize Annotations Dictionary and Counters\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "annotations = {}\n",
        "activity\\_distribution = Counter()\n",
        "total\\_images = 0\n",
        "valid\\_images = 0\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Get List of Image Files\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "image\\_files = \\[f for f in os.listdir(image\\_dir)\n",
        "if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Loop Over Images to Generate Pseudo-Labels with Progress Bar\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "for img\\_name in tqdm(image\\_files, desc=\"Processing images\", unit=\"image\"):\n",
        "img\\_path = os.path.join(image\\_dir, img\\_name)\n",
        "try:\n",
        "image = Image.open(img\\_path).convert(\"RGB\")\n",
        "total\\_images += 1\n",
        "except Exception as e:\n",
        "print(f\"\\nError opening {img\\_path}: {e}\")\n",
        "continue\n",
        "\n",
        "```\n",
        "# Preprocess the image for CLIP\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Compute image features and normalize them\n",
        "    image_features = model.encode_image(image_input)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # 1. Check if the image contains a single person\n",
        "    person_count_features = model.encode_text(person_count_tokens)\n",
        "    person_count_features /= person_count_features.norm(dim=-1, keepdim=True)\n",
        "    person_count_similarity = (100.0 * image_features @ person_count_features.T).softmax(dim=-1)\n",
        "    person_count_idx = person_count_similarity.argmax().item()\n",
        "\n",
        "    # Skip if not a single person\n",
        "    if person_count_idx != 0:  # 0 corresponds to \"a photo of a single person\"\n",
        "        continue\n",
        "\n",
        "    # 2. Check if the activity is clearly identifiable\n",
        "    clarity_features = model.encode_text(activity_clarity_tokens)\n",
        "    clarity_features /= clarity_features.norm(dim=-1, keepdim=True)\n",
        "    clarity_similarity = (100.0 * image_features @ clarity_features.T).softmax(dim=-1)\n",
        "    clarity_idx = clarity_similarity.argmax().item()\n",
        "\n",
        "    # Skip if activity is ambiguous\n",
        "    if clarity_idx != 0:  # 0 corresponds to \"a photo with a clearly identifiable human activity\"\n",
        "        continue\n",
        "\n",
        "    # 3. Classify the activity for valid images\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    activity_similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Get confidence scores for all activities\n",
        "    confidence_scores = activity_similarity[0].cpu().numpy()\n",
        "    best_idx = activity_similarity.argmax().item()\n",
        "    predicted_activity = activity_labels[best_idx]\n",
        "    confidence = float(confidence_scores[best_idx])\n",
        "\n",
        "    # Only consider as valid if confidence is above threshold\n",
        "    if confidence < 0.5:  # You can adjust this threshold\n",
        "        continue\n",
        "\n",
        "    # This is a valid image with a clearly identifiable activity\n",
        "    valid_images += 1\n",
        "    activity_distribution[predicted_activity] += 1\n",
        "\n",
        "# Save the pseudo-label for valid images in the annotations dictionary\n",
        "annotations[img_name] = {\n",
        "    'pseudo_label': predicted_activity,\n",
        "}\n",
        "```\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Save the Pseudo-Labels to a JSON File\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "with open(output\\_file, 'w') as f:\n",
        "json.dump(annotations, f, indent=4)\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Display Summary Statistics\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "print(f\"\\nAnnotation complete. Results saved to {output\\_file}\")\n",
        "print(f\"Total images processed: {total\\_images}\")\n",
        "print(f\"Valid images (single person with clear activity): {valid\\_images}\")\n",
        "print(\"\\nActivity Distribution:\")\n",
        "print(\"-\" \\* 40)\n",
        "\n",
        "# Sort activities by frequency (most common first)\n",
        "\n",
        "for activity, count in activity\\_distribution.most\\_common():\n",
        "percentage = (count / valid\\_images) \\* 100 if valid\\_images > 0 else 0\n",
        "print(f\"{activity}: {count} images ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Load the DensePose file\n",
        "\n",
        "densepose\\_file = 'C:/Users/devel/Desktop/DensePose/train2014\\_densepose\\_annotations.json'\n",
        "with open(densepose\\_file, 'r') as f:\n",
        "densepose\\_data = json.load(f)\n",
        "\n",
        "# Load the activity-annotated file\n",
        "\n",
        "activity\\_file = 'C:/Users/devel/Desktop/DensePose/train2014\\_activity\\_annotated.json'\n",
        "with open(activity\\_file, 'r') as f:\n",
        "activity\\_data = json.load(f)\n",
        "\n",
        "# Extract numeric image\\_ids from activity\\_data keys (e.g., 'COCO\\_train2014\\_000000401169.jpg' â†’ 401169)\n",
        "\n",
        "activity\\_image\\_ids = set()\n",
        "for key in activity\\_data.keys():\n",
        "\\# Use regex to extract the 12-digit number at the end of the filename (before .jpg)\n",
        "match = re.search(r'\\_(\\d{12}).jpg', key)\n",
        "if match:\n",
        "image\\_id = int(match.group(1))  # Convert to integer\n",
        "activity\\_image\\_ids.add(image\\_id)\n",
        "\n",
        "# Debugging: Print the extracted image\\_ids\n",
        "\n",
        "print(f\"First few numeric image\\_ids: {list(activity\\_image\\_ids)\\[:5]}\")\n",
        "\n",
        "# Extract UV maps from the DensePose annotations for images in the activity-annotated file\n",
        "\n",
        "uv\\_maps = \\[]\n",
        "for annotation in densepose\\_data\\['annotations']:\n",
        "if 'dp\\_U' in annotation and 'dp\\_V' in annotation and 'dp\\_I' in annotation:\n",
        "\\# Check if the image\\_id is in the activity-annotated image\\_ids\n",
        "if annotation\\['image\\_id'] in activity\\_image\\_ids:\n",
        "uv\\_map = {\n",
        "'image\\_id': annotation\\['image\\_id'],\n",
        "'dp\\_U': annotation\\['dp\\_U'],\n",
        "'dp\\_V': annotation\\['dp\\_V'],\n",
        "'dp\\_I': annotation\\['dp\\_I']\n",
        "}\n",
        "uv\\_maps.append(uv\\_map)\n",
        "\n",
        "# Save the extracted UV maps to a new file\n",
        "\n",
        "uv\\_maps\\_file = 'C:/Users/devel/Desktop/DensePose/train2014\\_extracted.json'\n",
        "with open(uv\\_maps\\_file, 'w') as f:\n",
        "json.dump(uv\\_maps, f)\n",
        "\n",
        "print(f\"Filtered {len(uv\\_maps)} UV maps and saved them to {uv\\_maps\\_file}.\")"
      ],
      "metadata": {
        "id": "hqbSXdVSICAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def augment\\_uv\\_map(uv\\_map):\n",
        "rotation\\_angle = np.random.uniform(-30, 30)\n",
        "cos\\_theta = np.cos(np.radians(rotation\\_angle))\n",
        "sin\\_theta = np.sin(np.radians(rotation\\_angle))\n",
        "\n",
        "```\n",
        "u = np.array(uv_map['dp_U'])\n",
        "v = np.array(uv_map['dp_V'])\n",
        "\n",
        "u_rotated = cos_theta * u - sin_theta * v\n",
        "v_rotated = sin_theta * u + cos_theta * v\n",
        "\n",
        "u_rotated = np.clip(u_rotated, 0, 1)\n",
        "v_rotated = np.clip(v_rotated, 0, 1)\n",
        "\n",
        "return {\n",
        "    'image_id': uv_map['image_id'],\n",
        "    'dp_U': u_rotated.tolist(),\n",
        "    'dp_V': v_rotated.tolist(),\n",
        "    'dp_I': uv_map['dp_I']\n",
        "}\n",
        "```\n",
        "\n",
        "# Hardcoded file paths\n",
        "\n",
        "input\\_file = 'C:/Users/devel/Desktop/DensePose/train2014\\_extracted.json'\n",
        "output\\_file = 'C:/Users/devel/Desktop/DensePose/train2014\\_extracted\\_augmented.json'\n",
        "\n",
        "# Load input data\n",
        "\n",
        "print(f\"Loading input file: {input\\_file}\")\n",
        "with open(input\\_file, 'r') as f:\n",
        "uv\\_maps = json.load(f)\n",
        "\n",
        "# Process with progress tracking\n",
        "\n",
        "print(\"Augmenting UV maps...\")\n",
        "augmented\\_uv\\_maps = \\[]\n",
        "for uv\\_map in tqdm(uv\\_maps, desc=\"Processing\", unit=\" map\"):\n",
        "augmented = augment\\_uv\\_map(uv\\_map)\n",
        "augmented\\_uv\\_maps.append(augmented)\n",
        "\n",
        "# Save output\n",
        "\n",
        "print(f\"\\nSaving augmented data to: {output\\_file}\")\n",
        "with open(output\\_file, 'w') as f:\n",
        "json.dump(augmented\\_uv\\_maps, f)\n",
        "\n",
        "print(f\"Completed! Processed {len(augmented\\_uv\\_maps)} UV maps.\")"
      ],
      "metadata": {
        "id": "0oBSoMs6IRJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random\\_split\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# Dataset Definition with Optional DensePose Mask\n",
        "\n",
        "\\########################################\n",
        "\n",
        "class ActivityDensePoseDataset(Dataset):\n",
        "def **init**(self, images\\_dir, activity\\_annotation\\_path, densepose\\_annotation\\_path, transform=None, use\\_densepose\\_mask=False):\n",
        "\"\"\"\n",
        "Args:\n",
        "images\\_dir (str): Directory containing the images.\n",
        "activity\\_annotation\\_path (str): Path to train2014\\_activity\\_annotated.json.\n",
        "densepose\\_annotation\\_path (str): Path to train2014\\_extracted\\_augmented.json.\n",
        "transform (callable, optional): Optional transform to apply to images.\n",
        "use\\_densepose\\_mask (bool): When True, generates a DensePose mask to stack with image.\n",
        "\"\"\"\n",
        "self.images\\_dir = images\\_dir\n",
        "self.transform = transform\n",
        "self.use\\_densepose\\_mask = use\\_densepose\\_mask\n",
        "\n",
        "```\n",
        "    # Load activity annotations (mapping: image filename â†’ { \"pseudo_label\": ... } )\n",
        "    with open(activity_annotation_path, 'r') as f:\n",
        "        self.activity_annotations = json.load(f)\n",
        "\n",
        "    # Load DensePose annotations (list of dictionaries)\n",
        "    with open(densepose_annotation_path, 'r') as f:\n",
        "        densepose_list = json.load(f)\n",
        "\n",
        "    # Build mapping: image filename â†’ list of DensePose annotations.\n",
        "    self.densepose_annotations = {}\n",
        "    for ann in densepose_list:\n",
        "        # If annotation is stored as a string, convert it.\n",
        "        if isinstance(ann, str):\n",
        "            try:\n",
        "                ann = json.loads(ann.strip())\n",
        "            except Exception:\n",
        "                continue\n",
        "        try:\n",
        "            image_id = int(ann[\"image_id\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "        # Format image filename (zero-padded to 12 digits).\n",
        "        filename = f\"COCO_train2014_{image_id:012d}.jpg\"\n",
        "        if filename not in self.densepose_annotations:\n",
        "            self.densepose_annotations[filename] = []\n",
        "        self.densepose_annotations[filename].append(ann)\n",
        "\n",
        "    # List of image filenames from the activity annotations.\n",
        "    self.image_files = list(self.activity_annotations.keys())\n",
        "\n",
        "    # Mapping of activity text to integer label.\n",
        "    self.label_mapping = {\n",
        "        \"standing\": 0,\n",
        "        \"sitting\": 1,\n",
        "        \"lying down\": 2,\n",
        "        \"bending\": 3,\n",
        "        \"jumping\": 4,\n",
        "        \"walking\": 5,\n",
        "        \"running\": 6\n",
        "    }\n",
        "\n",
        "def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "    image_file = self.image_files[idx]\n",
        "    image_path = os.path.join(self.images_dir, image_file)\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    activity_info = self.activity_annotations[image_file]\n",
        "    pseudo_label = activity_info.get(\"pseudo_label\", \"\").lower()\n",
        "    label = self.label_mapping.get(pseudo_label, -1)\n",
        "    densepose_data = self.densepose_annotations.get(image_file, [])\n",
        "    if self.use_densepose_mask:\n",
        "        mask = generate_densepose_mask(densepose_data, output_size=(image.shape[1], image.shape[2]))\n",
        "        # Concatenate the mask (1 channel) with the image (3 channels) to form a 4-channel tensor.\n",
        "        image = torch.cat([image, mask], dim=0)\n",
        "    return {\"image\": image, \"label\": label, \"densepose\": densepose_data}\n",
        "```\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# DensePose Mask Generator Function\n",
        "\n",
        "\\########################################\n",
        "\n",
        "def generate\\_densepose\\_mask(densepose\\_data, output\\_size=(224,224)):\n",
        "\"\"\"\n",
        "Generate a single-channel mask from DensePose annotations.\n",
        "For each annotated point (with normalized coordinates), mark a pixel with 1.0,\n",
        "then apply Gaussian blur to smooth.\n",
        "\"\"\"\n",
        "mask = np.zeros(output\\_size, dtype=np.float32)\n",
        "for ann in densepose\\_data:\n",
        "dp\\_U = ann.get('dp\\_U', \\[])\n",
        "dp\\_V = ann.get('dp\\_V', \\[])\n",
        "for u, v in zip(dp\\_U, dp\\_V):\n",
        "\\# Map normalized coordinates \\[0,1] to pixel coordinates.\n",
        "x = int(u \\* output\\_size\\[1])\n",
        "y = int(v \\* output\\_size\\[0])\n",
        "x = np.clip(x, 0, output\\_size\\[1]-1)\n",
        "y = np.clip(y, 0, output\\_size\\[0]-1)\n",
        "mask\\[y, x] = 1.0\n",
        "mask = cv2.GaussianBlur(mask, (7, 7), 0)\n",
        "mask = torch.from\\_numpy(mask).unsqueeze(0)  # Shape: (1, H, W)\n",
        "return mask\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# Custom Collate Function\n",
        "\n",
        "\\########################################\n",
        "\n",
        "def custom\\_collate(batch):\n",
        "\\# Stack image tensors and labels; keep densepose lists as-is.\n",
        "images = torch.stack(\\[item\\[\"image\"] for item in batch], dim=0)\n",
        "labels = torch.tensor(\\[item\\[\"label\"] for item in batch])\n",
        "densepose = \\[item\\[\"densepose\"] for item in batch]\n",
        "return {\"image\": images, \"label\": labels, \"densepose\": densepose}\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# Phase 1: Training with Raw Images (3 channels)\n",
        "\n",
        "\\########################################\n",
        "\n",
        "def train\\_raw\\_model(dataset, num\\_epochs=5, batch\\_size=16, learning\\_rate=1e-3, val\\_split=0.2):\n",
        "\\# Split dataset into training and validation sets.\n",
        "dataset\\_size = len(dataset)\n",
        "val\\_size = int(val\\_split \\* dataset\\_size)\n",
        "train\\_size = dataset\\_size - val\\_size\n",
        "train\\_dataset, val\\_dataset = random\\_split(dataset, \\[train\\_size, val\\_size])\n",
        "\n",
        "```\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=custom_collate)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
        "\n",
        "# Load pretrained ResNet-50 using the updated API.\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, 7)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / train_size\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validating\", leave=False):\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "return model\n",
        "```\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# Phase 2: Transfer Learning using DensePose Data\n",
        "\n",
        "\\########################################\n",
        "\n",
        "def modify\\_model\\_for\\_densepose(model):\n",
        "\"\"\"\n",
        "Modify the first conv layer to accept 4-channel input.\n",
        "Initialize the extra channel weights as the average of the first three channels.\n",
        "\"\"\"\n",
        "old\\_conv = model.conv1\n",
        "new\\_conv = nn.Conv2d(4,\n",
        "old\\_conv.out\\_channels,\n",
        "kernel\\_size=old\\_conv.kernel\\_size,\n",
        "stride=old\\_conv.stride,\n",
        "padding=old\\_conv.padding,\n",
        "bias=old\\_conv.bias is not None)\n",
        "with torch.no\\_grad():\n",
        "new\\_conv.weight\\[:, :3] = old\\_conv.weight\n",
        "new\\_conv.weight\\[:, 3] = old\\_conv.weight.mean(dim=1)\n",
        "model.conv1 = new\\_conv\n",
        "return model\n",
        "\n",
        "def fine\\_tune\\_with\\_densepose(dataset, base\\_model, num\\_epochs=5, batch\\_size=16, learning\\_rate=1e-4, val\\_split=0.2):\n",
        "dataset\\_size = len(dataset)\n",
        "val\\_size = int(val\\_split \\* dataset\\_size)\n",
        "train\\_size = dataset\\_size - val\\_size\n",
        "train\\_dataset, val\\_dataset = random\\_split(dataset, \\[train\\_size, val\\_size])\n",
        "\n",
        "```\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=custom_collate)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=custom_collate)\n",
        "\n",
        "# Modify the base model to accept 4-channel input.\n",
        "model = modify_model_for_densepose(base_model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} DensePose Training\", leave=False):\n",
        "        images = batch[\"image\"].to(device)  # 4-channel input now.\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / train_size\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} DensePose Validating\", leave=False):\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "    val_acc = correct / total\n",
        "    print(f\"(DensePose FT) Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "return model\n",
        "```\n",
        "\n",
        "\\########################################\n",
        "\n",
        "# Main Execution\n",
        "\n",
        "\\########################################\n",
        "\n",
        "if **name** == \"**main**\":\n",
        "\\# Update your paths.\n",
        "images\\_directory = \"C:/Users/devel/Desktop/DensePose/train2014/train2014\"\n",
        "activity\\_json = \"C:/Users/devel/Desktop/DensePose/train2014\\_activity\\_annotated.json\"\n",
        "densepose\\_json = \"C:/Users/devel/Desktop/DensePose/train2014\\_extracted\\_augmented.json\"\n",
        "\n",
        "```\n",
        "# Define a transformation (resize to 224x224 and convert to tensor).\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# --- Phase 1: Train with Raw Images ---\n",
        "print(\"=== Phase 1: Training on Raw Images ===\")\n",
        "raw_dataset = ActivityDensePoseDataset(images_directory, activity_json, densepose_json,\n",
        "                                         transform=transform, use_densepose_mask=False)\n",
        "raw_model = train_raw_model(raw_dataset, num_epochs=5, batch_size=16, learning_rate=1e-3)\n",
        "\n",
        "# --- Phase 2: Fine-tune with DensePose Data ---\n",
        "print(\"=== Phase 2: Fine-tuning with DensePose Data ===\")\n",
        "densepose_dataset = ActivityDensePoseDataset(images_directory, activity_json, densepose_json,\n",
        "                                               transform=transform, use_densepose_mask=True)\n",
        "densepose_model = fine_tune_with_densepose(densepose_dataset, base_model=raw_model,\n",
        "                                           num_epochs=5, batch_size=16, learning_rate=1e-4)\n",
        "\n",
        "# Optionally, save the trained models.\n",
        "torch.save(raw_model.state_dict(), \"raw_model.pth\")\n",
        "torch.save(densepose_model.state_dict(), \"densepose_model.pth\")\n",
        "```"
      ],
      "metadata": {
        "id": "T1cxUotgIZWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}