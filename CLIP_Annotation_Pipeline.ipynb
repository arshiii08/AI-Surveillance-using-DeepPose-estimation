{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import clip  # OpenAI CLIP package\n",
        "from tqdm import tqdm  # For progress bar with ETA\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------\n",
        "# Setup: Paths and Device\n",
        "# -------------------------\n",
        "image_dir = '/root/Downloads/Human Action Recognition/test'                # Folder containing your images (update as needed)\n",
        "output_file = 'harTest_annotated.json'  # Output JSON file for pseudo-labels\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# Load the CLIP Model\n",
        "# -------------------------\n",
        "# We use the \"ViT-B/32\" CLIP model along with its preprocessing function.\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "model.eval()\n",
        "\n",
        "# -------------------------\n",
        "# Define Candidate Activity Labels\n",
        "# -------------------------\n",
        "# Update this list with your desired activity categories.\n",
        "activity_labels = [\n",
        "    \"standing\",\n",
        "    \"sitting\",\n",
        "    \"walking\",\n",
        "    \"running\",\n",
        "    \"jumping\",\n",
        "    \"lying down\",\n",
        "    \"bending\"\n",
        "]\n",
        "\n",
        "# Create text prompts for zero-shot classification\n",
        "text_prompts = [f\"a photo of a person {label}\" for label in activity_labels]\n",
        "text_tokens = clip.tokenize(text_prompts).to(device)\n",
        "\n",
        "# Create person count detection prompts\n",
        "person_count_prompts = [\n",
        "    \"a photo of a single person\",\n",
        "    \"a photo of multiple people\",\n",
        "    \"a photo with no people\"\n",
        "]\n",
        "person_count_tokens = clip.tokenize(person_count_prompts).to(device)\n",
        "\n",
        "# Create activity clarity prompts\n",
        "activity_clarity_prompts = [\n",
        "    \"a photo with a clearly identifiable human activity\",\n",
        "    \"a photo with an ambiguous or unclear human activity\"\n",
        "]\n",
        "activity_clarity_tokens = clip.tokenize(activity_clarity_prompts).to(device)\n",
        "\n",
        "# -------------------------\n",
        "# Initialize Annotations Dictionary and Counters\n",
        "# -------------------------\n",
        "annotations = {}\n",
        "activity_distribution = Counter()\n",
        "total_images = 0\n",
        "valid_images = 0\n",
        "\n",
        "# -------------------------\n",
        "# Get List of Image Files\n",
        "# -------------------------\n",
        "image_files = [f for f in os.listdir(image_dir)\n",
        "               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "# -------------------------\n",
        "# Loop Over Images to Generate Pseudo-Labels with Progress Bar\n",
        "# -------------------------\n",
        "for img_name in tqdm(image_files, desc=\"Processing images\", unit=\"image\"):\n",
        "    img_path = os.path.join(image_dir, img_name)\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        total_images += 1\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError opening {img_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Preprocess the image for CLIP\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Compute image features and normalize them\n",
        "        image_features = model.encode_image(image_input)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # 1. Check if the image contains a single person\n",
        "        person_count_features = model.encode_text(person_count_tokens)\n",
        "        person_count_features /= person_count_features.norm(dim=-1, keepdim=True)\n",
        "        person_count_similarity = (100.0 * image_features @ person_count_features.T).softmax(dim=-1)\n",
        "        person_count_idx = person_count_similarity.argmax().item()\n",
        "\n",
        "        # Skip if not a single person\n",
        "        if person_count_idx != 0:  # 0 corresponds to \"a photo of a single person\"\n",
        "            continue\n",
        "\n",
        "        # 2. Check if the activity is clearly identifiable\n",
        "        clarity_features = model.encode_text(activity_clarity_tokens)\n",
        "        clarity_features /= clarity_features.norm(dim=-1, keepdim=True)\n",
        "        clarity_similarity = (100.0 * image_features @ clarity_features.T).softmax(dim=-1)\n",
        "        clarity_idx = clarity_similarity.argmax().item()\n",
        "\n",
        "        # Skip if activity is ambiguous\n",
        "        if clarity_idx != 0:  # 0 corresponds to \"a photo with a clearly identifiable human activity\"\n",
        "            continue\n",
        "\n",
        "        # 3. Classify the activity for valid images\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        activity_similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        # Get confidence scores for all activities\n",
        "        confidence_scores = activity_similarity[0].cpu().numpy()\n",
        "        best_idx = activity_similarity.argmax().item()\n",
        "        predicted_activity = activity_labels[best_idx]\n",
        "        confidence = float(confidence_scores[best_idx])\n",
        "\n",
        "        # Only consider as valid if confidence is above threshold\n",
        "        if confidence < 0.5:  # You can adjust this threshold\n",
        "            continue\n",
        "\n",
        "        # This is a valid image with a clearly identifiable activity\n",
        "        valid_images += 1\n",
        "        activity_distribution[predicted_activity] += 1\n",
        "\n",
        "    # Save the pseudo-label for valid images in the annotations dictionary\n",
        "    annotations[img_name] = {\n",
        "        'pseudo_label': predicted_activity,\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Save the Pseudo-Labels to a JSON File\n",
        "# -------------------------\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(annotations, f, indent=4)\n",
        "\n",
        "# -------------------------\n",
        "# Display Summary Statistics\n",
        "# -------------------------\n",
        "print(f\"\\nAnnotation complete. Results saved to {output_file}\")\n",
        "print(f\"Total images processed: {total_images}\")\n",
        "print(f\"Valid images (single person with clear activity): {valid_images}\")\n",
        "print(\"\\nActivity Distribution:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Sort activities by frequency (most common first)\n",
        "for activity, count in activity_distribution.most_common():\n",
        "    percentage = (count / valid_images) * 100 if valid_images > 0 else 0\n",
        "    print(f\"{activity}: {count} images ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "id": "k-RM6YThsGrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}